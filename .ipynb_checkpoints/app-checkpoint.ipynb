{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16a415f-5c31-4f30-8b4e-541dd49bcf24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from flask import Flask, request, render_template, render_template_string, send_file\n",
    "import pandas as pd\n",
    "import os\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "import threading\n",
    "import re\n",
    "import json\n",
    "from together import Together\n",
    "import openai  \n",
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from itertools import chain\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from time import sleep\n",
    "import time\n",
    "import unicodedata\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0aee31b-7a5d-4b76-94e8-ea3f88632b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"api_key\")\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") \n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b6dbd-0183-43a4-981e-e9ef30ebbfe5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1- AVAILABLE LLM WITH FALLBACK LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d6a91a6-b85b-4b70-b3fe-75633e477e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ai_model(prompt):  \n",
    "    # Try OpenRouter\n",
    "    model = \"\"\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "          base_url=\"https://openrouter.ai/api/v1\",\n",
    "          api_key= OPENROUTER_API_KEY,\n",
    "        ) \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "            messages=[{\"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": prompt}]}])\n",
    "        # print(\"✅ Used OpenRouter\")\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"⚠️ OpenRouter AI failed\")\n",
    "        print (\"\")\n",
    "\n",
    "    # Try Groq\n",
    "    try:\n",
    "        groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"compound-beta-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=1,\n",
    "            max_completion_tokens=3000,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None\n",
    "        )\n",
    "        # print(\"✅ Used Groq\")\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"⚠️ Groq failed: {e}\")\n",
    "        print (\"\")\n",
    "\n",
    "    # Try Together AI\n",
    "    try:\n",
    "        os.environ[\"TOGETHER_API_KEY\"] = TOGETHER_API_KEY\n",
    "        together_client = Together()\n",
    "        response = together_client.chat.completions.create(\n",
    "            model=\"deepseek-ai/DeepSeek-V3\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        # print(\"✅ Used Together AI\")\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ All models failed: {e}\")\n",
    "        return \"Sorry, no AI model is available right now.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c7a39-b63a-4239-b428-74934f2d7cb2",
   "metadata": {},
   "source": [
    "# 2- LINKS OF THE SUPPLIERS WITH SERPER API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7ac21-e829-4779-9779-6587ea78da9d",
   "metadata": {},
   "source": [
    "# 2-1 Serper Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40cc0090-23ff-4377-905c-38295ebafd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(url, headers, q, max_results):\n",
    "    # JSON payload with the search query q\n",
    "    payload = {\"q\": q}\n",
    "    try:\n",
    "        # API call to url serper with json query\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        # Extract the list of organic search results (ignores ads or sponsored content) and limits them to max_results.\n",
    "        organic_results = data.get(\"organic\", [])[:max_results]\n",
    "\n",
    "        # Liste des domaines / extensions à exclure\n",
    "        excluded_domains = [\"facebook.com\", \"instagram.com\", \"linkedin.com\", \"youtube.com\", \"twitter.com\"]\n",
    "        excluded_extensions = [\".pdf\", \".doc\", \".xls\", \".xlsx\", \".ppt\", \".pptx\"]\n",
    "\n",
    "        filtered_results = []\n",
    "        for item in organic_results:\n",
    "            link = item.get(\"link\", \"\").lower()\n",
    "\n",
    "            # Exclure si extension interdite\n",
    "            if any(link.endswith(ext) for ext in excluded_extensions):\n",
    "                continue\n",
    "\n",
    "            # Exclure si domaine interdit (on regarde si l'URL contient le domaine)\n",
    "            if any(domain in link for domain in excluded_domains):\n",
    "                continue\n",
    "\n",
    "            filtered_results.append({\"title\": item.get(\"title\", \"\"), \"href\": item.get(\"link\", \"\")})\n",
    "\n",
    "            # Stopper dès qu'on atteint max_results après filtrage\n",
    "            if len(filtered_results) >= max_results:\n",
    "                break\n",
    "\n",
    "        return filtered_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Serper API call failed:\", e)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644682fc-7c90-45ea-81a7-0d3d8bd125d6",
   "metadata": {},
   "source": [
    "# 2-2 Test scrapability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa32fb97-9fbb-48ff-8cc5-9a7c7b921592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scrapable(url, min_text_length=100, min_tags=5):\n",
    "    \"\"\"Détermine si une page nécessite JS (Selenium) ou peut être scrapée par requests.\n",
    "\n",
    "    Retourne :\n",
    "    - \"requests\" si scrapable sans JS,\n",
    "    - \"selenium\" si JS nécessaire,\n",
    "    - \"impossible\" en cas d’erreur ou d’échec.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    def fetch(url, timeout):\n",
    "        return requests.get(url, timeout=timeout, headers=headers)\n",
    "\n",
    "    try:\n",
    "        # Premier essai timeout court\n",
    "        response = fetch(url, timeout=5)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.warning(f\"❌ {url} returned status {response.status_code}\")\n",
    "            return \"impossible\"\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"⏳ Timeout 5s sur {url}, nouvelle tentative avec timeout 10s...\")\n",
    "        try:\n",
    "            response = fetch(url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                logger.warning(f\"❌ {url} returned status {response.status_code} après retry\")\n",
    "                return \"impossible\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"⚠️ Erreur d'accès à {url} après retry 10s: {e}\")\n",
    "            return \"impossible\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"⚠️ Erreur d'accès à {url}: {e}\")\n",
    "        return \"impossible\"\n",
    "\n",
    "    # Analyse du contenu\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    visible_text = soup.get_text(strip=True)\n",
    "    tag_count = len(soup.find_all([\"div\", \"p\", \"span\", \"section\"]))\n",
    "\n",
    "    if len(visible_text) < min_text_length or tag_count < min_tags:\n",
    "        logger.info(f\"⚠️ {url} semble nécessiter JavaScript → Selenium recommandé.\")\n",
    "        return \"selenium\"\n",
    "\n",
    "    logger.info(f\"✅ {url} est scrapable sans JavaScript.\")\n",
    "    return \"requests\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478745b6-435c-4a0d-bdda-fe031eec6e96",
   "metadata": {},
   "source": [
    "# 2-3 Main Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f47087e-558a-4194-9923-b0a7513079db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_serper(input_ma, input_com, max_domain): \n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    headers = {\n",
    "        \"X-API-KEY\": SERPER_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    preferred_sites = [\"alstena.com\", \"aval.ma\", \"abratex.ma\", \"sdfi.ma\", \"ogicom.ma\",\n",
    "                       \"polindus.ma\", \"ft2e.ma\", \"cfimmaroc.com\", \"mtsindustrie.ma\"]\n",
    "\n",
    "    site_query = \" OR \".join([f\"site:{site}\" for site in preferred_sites])\n",
    "    prioritized_query = f\"{input_ma} ({site_query})\"\n",
    "\n",
    "    # query_fr = f\"{input_com} site:.fr\"\n",
    "    query_ma = f\"{input_com} site:.ma\"\n",
    "    \n",
    "    seen_sites = set()\n",
    "    filtered_results = []\n",
    "    deduplicated_results = []\n",
    "\n",
    "    results_pr = perform_search(url, headers, prioritized_query, max_domain)\n",
    "    results_ma = perform_search(url, headers, query_ma, max_domain)\n",
    "    # results_fr = perform_search(url, headers, query_fr, max_domain)\n",
    "    max_domain = max_domain*2\n",
    "    results_others = perform_search(url, headers, input_com, max_domain)\n",
    "    all_results = results_pr + results_ma + results_others\n",
    "\n",
    "    print(\"✅ All URLs Serper Search Results :\", len (all_results))\n",
    "    for u in all_results:\n",
    "        print(u[\"href\"])\n",
    "    \n",
    "# Checking scrapability\n",
    "    print(\"🔍 Checking scrapability for search results...\")\n",
    "    for result in all_results:\n",
    "        href = result.get(\"href\")\n",
    "        mode = is_scrapable(href)\n",
    "        if mode == \"requests\":\n",
    "            filtered_results.append(result)\n",
    "        elif mode == \"selenium\":\n",
    "            # html, text = scrape_with_selenium(url)\n",
    "            print(f\"YOU HAVE TO USE SELENIUM\")\n",
    "        else:\n",
    "            print(\"URL not accessible\")\n",
    "# Avoid redundance\n",
    "    for resulta in filtered_results:\n",
    "        href = resulta.get(\"href\")\n",
    "        parsed = urlparse(href)\n",
    "        base_url = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        if base_url not in seen_sites:\n",
    "            seen_sites.add(base_url)\n",
    "            deduplicated_results.append(resulta)\n",
    "\n",
    "    print(\"✅ Filtered URLs with total :\", len (filtered_results))\n",
    "    print(\"✅ Filtered and deduplicated URLs ready for scraping with total :\", len (deduplicated_results))\n",
    "    for r in deduplicated_results:\n",
    "        print(r[\"href\"])\n",
    "\n",
    "    return deduplicated_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f8d2f0-8685-4b7c-a5bb-e5b4bddcd4fd",
   "metadata": {},
   "source": [
    "# 3-EXTRACT INFO FROM HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b89c0-1ad0-4d22-b33f-166d9e35fd86",
   "metadata": {},
   "source": [
    "# 3-1 Kerix.net Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69f6a9f9-448f-4cbe-b31f-9e91cc38a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_query(search_query):\n",
    "    # Convert to lowercase\n",
    "    query = search_query.lower()\n",
    "\n",
    "    # Remove common French prepositions/articles\n",
    "    query = re.sub(r\"\\b(de|du|des|la|le|l'|d'|au|aux|en|et)\\b\", \"\", query)\n",
    "\n",
    "    # Normalize accents: ç → c, é → e, etc.\n",
    "    query = unicodedata.normalize('NFKD', query).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Replace remaining non-alphanum with dashes\n",
    "    query = re.sub(r\"[’'\\s]+\", \"-\", query)\n",
    "\n",
    "    # Remove multiple consecutive dashes\n",
    "    query = re.sub(r\"-+\", \"-\", query)\n",
    "\n",
    "    # Strip leading/trailing dashes\n",
    "    return query.strip(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bc1bbef-7bdd-433e-abf7-ab1752c41b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kerix_phone(soup):\n",
    "    \"\"\"Extract Moroccan phone numbers from page\"\"\"\n",
    "    phone_text = soup.get_text()\n",
    "    phones = re.findall(r\"(?:\\+212|0)[\\s-]?[5-7]\\d[\\s-]?\\d{3}[\\s-]?\\d{3}\", phone_text)\n",
    "    return phones[0] if phones else None\n",
    "\n",
    "def extract_kerix_email(soup):\n",
    "    \"\"\"Extract email from contact section\"\"\"\n",
    "    for link in soup.select('a[href^=\"mailto:\"]'):\n",
    "        if \"@\" in link.get_text():\n",
    "            return link.get_text().strip()\n",
    "    return None\n",
    "\n",
    "def extract_kerix_address(soup):\n",
    "    \"\"\"Extract address from Kerix.net profile pages\"\"\"\n",
    "    # Method 1: Check the specific address container\n",
    "    if address_div := soup.select_one('div.col-lg-6 p.card-text'):\n",
    "        # Clean up the address text\n",
    "        address = ' '.join(address_div.get_text(strip=True, separator=' ').split())\n",
    "        return address\n",
    "    \n",
    "    # Method 2: Fallback to general card text search\n",
    "    for p in soup.select('p.card-text'):\n",
    "        text = p.get_text(strip=True)\n",
    "        if any(x in text.lower() for x in [\"bd\", \"rue\", \"av.\", \"casablanca\", \"maroc\"]):\n",
    "            return ' '.join(text.split())  # Normalize whitespace\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6560bd18-604e-457a-ac9d-da7721af5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kerix_contacts(search_query, max_links, delay=3):\n",
    "\n",
    "    base_url = \"https://www.kerix.net\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9\",\n",
    "    }\n",
    "\n",
    "    # Step 1: Search page\n",
    "    # formatted_query = re.sub(r\"[’'\\s]\", \"-\", search_query.lower())\n",
    "    formatted_query = normalize_query(search_query)\n",
    "    search_url = f\"{base_url}/fr/annuaire-entreprise/{formatted_query}.html\"\n",
    "    \n",
    "    try:\n",
    "        time.sleep(delay)\n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Kerix Search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Step 2: Extract company links using PRECISE selectors\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    company_links = []\n",
    "    \n",
    "    # NEW: Correct selector based on actual HTML\n",
    "    for card in soup.select('div.card.mt-2'):  # Matches the company cards exactly\n",
    "        # Method 1: Get link from title\n",
    "        if title := card.select_one('h5.card-title a[href*=\"/fr/annuaire-entreprise/\"]'):\n",
    "            company_links.append(urljoin(base_url, title['href']))\n",
    "        \n",
    "        # Method 2: Alternative fallback (the \"Voir plus\" button)\n",
    "        elif voir_plus := card.select_one('a.btn-success[href*=\"/fr/annuaire-entreprise/\"]'):\n",
    "            company_links.append(urljoin(base_url, voir_plus['href']))\n",
    "        if len(company_links) >= max_links:\n",
    "            break\n",
    "\n",
    "    # Step 3: Scrape company pages with contact extraction\n",
    "    results = []\n",
    "    for url in company_links:\n",
    "        try:\n",
    "            time.sleep(delay)\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            company_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract key information\n",
    "            company_info = {\n",
    "                \"part_number\": \"Not identified\",\n",
    "                \"name\": company_soup.select_one('h1').get_text(strip=True) if company_soup.select_one('h1') else None,\n",
    "                \"email\": extract_kerix_email(company_soup),\n",
    "                \"phone\": extract_kerix_phone(company_soup),              \n",
    "                \"address\": extract_kerix_address(company_soup),\n",
    "                \"price\": \"Demander un devis\",\n",
    "                \"url\": url,\n",
    "            }\n",
    "            results.append(company_info)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to process {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Final Output\n",
    "    if results:\n",
    "        print(\"\\n📋 Résultats extraits depuis Kerix.net :\")\n",
    "        for res in results:\n",
    "            print(\"-\" * 80)\n",
    "            for key, value in res.items():\n",
    "                print(f\"{key.capitalize():<10}: {value}\")\n",
    "    else:\n",
    "        print(\"❗ Aucun résultat trouvé sur Kerix.net.\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4565c-1b6d-4648-aff1-6e1877e50cd6",
   "metadata": {},
   "source": [
    "# 3-2 Using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbe4e93d-c496-4774-88c7-a8813f63101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_supplier_info_from_html(text, url):\n",
    "    prompt = f\"\"\"\n",
    "You are a structured information extraction agent. Analyze the following extracted text from an industrial materials or tools web page ({url}) and extract the supplier and product information in **strict JSON format**.\n",
    "\n",
    "Extract and return these exact fields:\n",
    "{{\n",
    "  \"part_number\": \"Product reference such as SKU, NSN, ref, model number, etc.\",\n",
    "  \"name\": \"Company or supplier name. If the name is not found, extract the domain name from the URL (e.g., 'example.com') and use it.\",\n",
    "  \"email\": \"Professional business email address\",\n",
    "  \"phone\": \"Phone or WhatsApp number with area/country code if available\",\n",
    "  \"address\": \"Physical or business address of the supplier\",\n",
    "  \"price\": \"Price of the product (with currency symbol or code if mentioned)\",\n",
    "  \"url\": \"Use the provided URL exactly as given\"\n",
    "}}\n",
    "\n",
    "⚠️ Extraction Guidelines:\n",
    "- Only include fields **explicitly found** in the text — do **not guess** or infer missing data.\n",
    "- If a field is **not present**, return it as an empty string \"\".\n",
    "- Do not include explanations, comments, or any extra text.\n",
    "- Focus only on supplier contact details and product identifiers/prices.\n",
    "- Ignore navigation menus, disclaimers, ads, or general company descriptions.\n",
    "\n",
    "Your response must be a **valid JSON object**. No markdown, no code blocks, and no human language output.\n",
    "\n",
    "Here is the extracted text:\n",
    "{text[:5000]}\n",
    "\"\"\"\n",
    "    content = call_ai_model(prompt)   \n",
    "    try:\n",
    "        json_str = re.search(r\"\\{.*\\}\", content, re.DOTALL).group()\n",
    "        resultat= json.loads(json_str)\n",
    "        logger.debug(f\"✅ voici le résultat de passage au LLM de l'Url: {url}\")\n",
    "        print (resultat)\n",
    "        return resultat\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to parse response for URL: {url}\\nRaw content:\\n{content}\\nError: {e}\")\n",
    "        return {\n",
    "            \"part_number\": \"\",\n",
    "            \"name\": \"\",\n",
    "            \"email\": \"\",\n",
    "            \"phone\": \"\",\n",
    "            \"address\": \"\",\n",
    "            \"price\": \"\",\n",
    "            \"url\": url\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42c08f-2787-400e-a0c7-d863a89715cd",
   "metadata": {},
   "source": [
    "# 4-INFO FROM QUERY USING LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98db2df5-3822-442f-bb25-50d6d5eb5463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vehicle_and_part(user_query):\n",
    "    prompt = f\"\"\"\n",
    "You are a multilingual smart assistant.\n",
    "\n",
    "The user is looking for an industrial material. Based on this request:\n",
    "\"{user_query}\"\n",
    "\n",
    "Return the result as a JSON object with the following fields:\n",
    "\n",
    "- \"category\": General category like \"tools\", \"equipment\", \"fasteners\", \"hydraulic parts\", etc.\n",
    "- \"item\": Specific English name of the material (e.g., \"torque wrench\", \"abrasive paper\")\n",
    "- \"item_french\": French translation of \"item\" (e.g., \"clé dynamométrique\", \"papier abrasif\")\n",
    "- \"context\": Optional detail like usage, target machine, or size.\n",
    "\n",
    "Return ONLY valid JSON, like this:\n",
    "{{\n",
    "  \"category\": \"Tools\",\n",
    "  \"item\": \"Abrasive paper\",\n",
    "  \"item_french\": \"Papier abrasif\",\n",
    "  \"context\": \"for automotive bodywork sanding\"\n",
    "}}\n",
    "If a field is not mentioned, return it as an empty string \"\".\n",
    "\"\"\"\n",
    "\n",
    "    content = call_ai_model(prompt)\n",
    "\n",
    "    # Clean code block formatting if present\n",
    "    content_clean = re.sub(r\"```json|```\", \"\", content)\n",
    "\n",
    "    try:\n",
    "        json_match = re.search(r\"\\{.*\\}\", content_clean, re.DOTALL)\n",
    "        if json_match:\n",
    "            parsed = json.loads(json_match.group())\n",
    "            item = parsed.get(\"item\", \"\")\n",
    "            item_french = parsed.get(\"item_french\", \"\")\n",
    "            category = parsed.get(\"category\", \"\")\n",
    "            context = parsed.get(\"context\", \"\")\n",
    "            # print(f\"item: {item} | item_french: {item_french} | category: {category} | context: {context}\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ JSON parsing failed. Raw content:\", content)\n",
    "        item = item_french = category = context = \"\"\n",
    "\n",
    "    return item, item_french, category, context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276cdfd5-ce51-4147-9ea6-e6ba5467f0ee",
   "metadata": {},
   "source": [
    "# 5-TEXT FROM HTML USING DOM TREE WITH BEAUTIFULSOUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b1f0355-829e-4f17-a24a-c17ed9e8a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevantText_from_HTML (sub_url):\n",
    "\n",
    "    sub_page = fetch_protected_page(sub_url)\n",
    "    soup = BeautifulSoup(sub_page.text, \"html.parser\")\n",
    "    \n",
    "    \n",
    "# ➤ DOM-Aware content extraction from main page\n",
    "    supplier_sections = []\n",
    "    price_sections = []\n",
    "    reference_sections = []\n",
    "\n",
    "    # Define contact-related keywords\n",
    "    contact_keywords = [\"contact\", \"info\", \"email\", \"mail\", \"phone\", \"tel\", \"adresse\", \"address\", \"whatsapp\"]\n",
    "    # Loop through candidate tags\n",
    "    for tag in [\"footer\", \"address\", \"header\", \"section\", \"div\"]:\n",
    "        for section in soup.find_all(tag):\n",
    "            classes = \" \".join(section.get(\"class\", [])).lower()\n",
    "            id_attr = section.get(\"id\", \"\").lower()\n",
    "            \n",
    "            # Only keep blocks where id/class contains contact-related info\n",
    "            if any(keyword in classes or keyword in id_attr for keyword in contact_keywords):\n",
    "                contact_section = section.get_text(separator=\" \", strip=True)\n",
    "                supplier_sections.append(contact_section)  \n",
    "    \n",
    "    # Search for price candidates\n",
    "    for tag in soup.find_all([\"div\", \"span\", \"p\", \"strong\"]):\n",
    "        class_name = \" \".join(tag.get(\"class\", []))\n",
    "        id_name = tag.get(\"id\", \"\")\n",
    "    \n",
    "        if any(keyword in class_name.lower() or keyword in id_name.lower() for keyword in [\"price\", \"cost\", \"amount\", \"tarif\", \"prix\"]):\n",
    "            price_text = tag.get_text(strip=True)\n",
    "            if any(c in price_text for c in \"$€£DH\") or any(char.isdigit() for char in price_text):        \n",
    "                price_sections.append(price_text)\n",
    "    \n",
    "    # Search for part number candidates\n",
    "    for tag in soup.find_all([\"div\", \"span\", \"p\", \"li\", \"td\", \"th\"]):\n",
    "        text = tag.get_text(strip=True).lower()\n",
    "        if any(kw in text for kw in [\"part number\", \"référence\", \"sku\", \"ref\", \"nsn\"]):\n",
    "            reference = tag.get_text(strip=True)\n",
    "            reference_sections.append(reference)\n",
    "    \n",
    "    all_sections = list(dict.fromkeys(chain(supplier_sections, price_sections, reference_sections)))\n",
    "\n",
    "    text = \"\\n\".join(all_sections)\n",
    "    # print (text)\n",
    "\n",
    "    return soup, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be661b82-ce0b-4e0c-bc3b-f305e8c6a283",
   "metadata": {},
   "source": [
    "# 6-FETCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7f3e343-72f9-48b2-b155-cba1db7327a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_query(item, category, context):\n",
    "\n",
    "    item_part = f\"{item} suppliers\" if item else \"industrial material suppliers\"\n",
    "    category_part = f\"in the {category} category\" if category else \"\"\n",
    "    context_part = f\"for {context}\" if context else \"for industrial applications\"\n",
    "\n",
    "    input_ma = item\n",
    "    # Assemble the full query\n",
    "    input_com = f\"{item_part}\"\n",
    "    if category_part:\n",
    "        input_com += f\" {category_part}\"\n",
    "    input_com += f\" {context_part}\"\n",
    "    \n",
    "    print(input_com)\n",
    "    return input_ma, input_com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6826ac-db4c-4820-9659-dec510cc0aef",
   "metadata": {},
   "source": [
    "# 6.1 Smart Direct Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e360d49-a882-4fe6-8166-692a71c144f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_request(url, headers=None, max_retries=5):\n",
    "    delay = 1\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"[{response.status_code}] Non-200 response. Retry...\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"❌ Error: {e}. Retry in {delay}s...\")\n",
    "\n",
    "        time.sleep(delay + random.uniform(0, 1))\n",
    "        delay *= 2  # Backoff exponentiel\n",
    "\n",
    "    print(\"❌ Échec après plusieurs tentatives.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106909da-77cb-4d7b-b921-6f72012e175a",
   "metadata": {},
   "source": [
    "# 6.2 Use of Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67be7aa0-519d-4c69-bebe-07453e907b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_selenium(url):\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    import time\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # wait for JS to load; adjust if needed\n",
    "\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        # Create a fake Response object\n",
    "        response = Response()\n",
    "        response.status_code = 200\n",
    "        response._content = html.encode(\"utf-8\")\n",
    "        response.url = url\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc2342-09c2-4072-8041-82e3c7a9b1d6",
   "metadata": {},
   "source": [
    "# 6.3 Unified fetching and Use of ScraperAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06a825d4-cf47-4253-b1b2-7c893717c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_protected_page(url, use_selenium=True):\n",
    "    \"\"\"\n",
    "    Try fetching a protected page:\n",
    "      1. Smart requests\n",
    "      2. ScraperAPI\n",
    "      3. Selenium (optional)\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # 1️⃣ Try direct smart request\n",
    "    response = smart_request(url, headers)\n",
    "    if response is not None:\n",
    "        return response\n",
    "\n",
    "\n",
    "    # 2️⃣ Try ScraperAPI\n",
    "    logger.info(\"⏱️ Passage à ScraperAPI...\")\n",
    "    payload = {\"api_key\": api_key, \"url\": url}\n",
    "    scraper_url = \"http://api.scraperapi.com\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(scraper_url, params=payload, timeout=15)\n",
    "        if resp.status_code == 200:\n",
    "            return resp\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ScraperAPI a échoué: {e}\")\n",
    "\n",
    "    # 3️⃣ Selenium fallback (optional)\n",
    "    if use_selenium:\n",
    "        logger.info(\"⚡ Passage à Selenium...\")\n",
    "        resp = scrape_with_selenium(url)\n",
    "        if resp and resp.status_code == 200:\n",
    "            return resp\n",
    "\n",
    "    # If all fails\n",
    "    logger.error(f\"❌ Impossible de récupérer {url}\")\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76b621-b0f2-42a8-9385-bdf9d8dbb922",
   "metadata": {},
   "source": [
    "# 7- GET THE LINKS AND SUBLINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cfbb16d-c108-4612-8495-4b3d5a462cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contact_links(soup, base_url):\n",
    "    # contact_links = {base_url}  # Avoid duplicates\n",
    "    contact_links = set()  # Avoid duplicates\n",
    "     \n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"].lower()\n",
    "        if any(kw in href for kw in [\"contact\", \"about\", \"a-propos\", \"entreprise\"]):\n",
    "            full_url = requests.compat.urljoin(base_url, href)\n",
    "            contact_links.add(full_url)\n",
    "    \n",
    "    contact_links_list = list(contact_links)\n",
    "    logger.debug(f\"✅ voici le principal Url: {base_url}\")\n",
    "    for link in contact_links_list[:4]:\n",
    "        print(f\"⚠️ voici Ses subUrl à analyser: {link}\")\n",
    "        \n",
    "    return contact_links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b3e3a-48d6-4124-96e5-0c5570584ef9",
   "metadata": {},
   "source": [
    "# 8-PARALLEL SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "823e546f-0612-4972-9694-ef5c77b7ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_supplier(url, max_subpages=4):\n",
    "    \"\"\"Scrape a supplier's main URL + subpages, aggregate text, and extract info once.\"\"\"\n",
    "\n",
    "    # Extract main page\n",
    "    soup, main_text = extract_relevantText_from_HTML(url)\n",
    "\n",
    "    # Find subpage links\n",
    "    contact_links = get_contact_links(soup, url)[:max_subpages]\n",
    "\n",
    "    # Extract texts from subpages (in parallel)\n",
    "    sub_texts = scrape_subpages_texts(contact_links)\n",
    "\n",
    "    # Combine all texts\n",
    "    all_texts = [main_text] + sub_texts\n",
    "    grouped_text = \"\\n\".join(all_texts)\n",
    "\n",
    "    # Call AI **once** with grouped text\n",
    "    supplier_info = extract_supplier_info_from_html(grouped_text, url)\n",
    "    supplier_info[\"url\"] = url  # Always include base URL\n",
    "\n",
    "    return supplier_info\n",
    "\n",
    "\n",
    "def scrape_subpages_texts(urls, delay=0.5, timeout=10):\n",
    "    \"\"\"Return list of texts extracted from subpages (no AI calls here).\"\"\"\n",
    "    texts = []\n",
    "    if not urls:\n",
    "        return texts\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(urls)) as executor:\n",
    "        futures = {}\n",
    "        for url in urls:\n",
    "            futures[executor.submit(extract_relevantText_from_HTML, url)] = url\n",
    "            sleep(delay)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                _, sub_text = future.result(timeout=timeout)\n",
    "                if sub_text:\n",
    "                    texts.append(sub_text)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Subpage scrape failed ({futures[future]}): {e}\")\n",
    "\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45a785-7ac2-4003-b956-fd79c08d93c3",
   "metadata": {},
   "source": [
    "# 8- INDEX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6adc6224-9ffe-4520-80e5-9283783ef050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logger\n",
    "logger = logging.getLogger(\"scraper_logger\")\n",
    "logger.setLevel(logging.DEBUG)  # Affiche tout, y compris les debug\n",
    "\n",
    "# Supprimer les handlers existants (important en Jupyter)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Handler pour afficher dans la console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Format des messages\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Ajout du handler au logger\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44db1d84-3d93-47e7-8a21-e3b8c90bbb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    global results\n",
    "    results = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        user_query = request.form[\"query\"]\n",
    "        logger.info(f\"🔍 Nouvelle requête reçue : {user_query}\")\n",
    "        \n",
    "        try:\n",
    "            item, item_french, category, context = extract_vehicle_and_part(user_query)\n",
    "            logger.debug(f\"Extraction réussie: item: {item} | item_french: {item_french} | category: {category} | context: {context}\")   \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur dans extract_vehicle_and_part: {e}\")\n",
    "            return render_template(\"error.html\", error=\"Erreur d’analyse de la requête.\")\n",
    "\n",
    "        try:\n",
    "            search_query_ma, search_query_com = build_search_query(item, category, context)\n",
    "            search_results = search_with_serper(search_query_ma, search_query_com, 5)\n",
    "            logger.info(f\"🧠 Résultats Serper récupérés: {len(search_results)} liens\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur dans search_with_serper: {e}\")\n",
    "            return render_template(\"error.html\", error=\"Erreur pendant la recherche Serper.\")\n",
    "\n",
    "        # Kerix.net scrapping\n",
    "        try:\n",
    "            contacts = get_kerix_contacts(item_french, max_links=5)\n",
    "            logger.info(f\"📞 Contacts Kerix récupérés: {len(contacts)}\")\n",
    "            results = contacts\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Échec récupération contacts Kerix: {e}\")\n",
    "\n",
    "        # Scraping des autres fournisseurs\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future_to_url = {\n",
    "                executor.submit(scrape_supplier, url[\"href\"]): url[\"href\"]\n",
    "                for url in search_results\n",
    "            }\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        # logger.info(f\"✅ Résultat trouvé pour {url}: {result}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Thread échoué pour {url}: {e}\")\n",
    "                    results.append({\n",
    "                        \"url\": url,\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "\n",
    "        # logger.info(f\"🎯 Total résultats collectés: {len(results)}\")\n",
    "        return render_template(\"result.html\", results=results, query=user_query)\n",
    "    \n",
    "    logger.debug(\"Page d’accueil chargée (GET)\")\n",
    "    return render_template(\"index1.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721d6a5-a63f-40d0-aa5e-ebc3acefbefa",
   "metadata": {},
   "source": [
    "# 9- EXPORT RESULTS TO EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ec6eb17-9098-4900-8983-ecb1cfe83044",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/download\")\n",
    "def download():\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df[[\"part_number\", \"name\", \"email\", \"phone\", \"address\", \"url\"]] \n",
    "    output = BytesIO()\n",
    "    df.to_excel(output, index=False)\n",
    "    output.seek(0)\n",
    "    return send_file(output, download_name=\"suppliers.xlsx\", as_attachment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2cbf6-95db-4e0c-ab1e-55e1625621fe",
   "metadata": {},
   "source": [
    "# 10- RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5c0bd8c-469e-4708-b4a4-61fc8b7d1263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5002\n",
      "Press CTRL+C to quit\n",
      "2025-09-02 19:24:10,050 - DEBUG - Page d’accueil chargée (GET)\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:12] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:17] \"GET /static/icon_intelligent.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:17] \"GET /static/icon_efficient.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:17] \"GET /static/icon_accurate.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:17] \"GET /static/logo5.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:17] \"GET /static/background9.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:24:18] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "2025-09-02 19:24:33,787 - INFO - 🔍 Nouvelle requête reçue : pompe centrifuge\n",
      "2025-09-02 19:26:28,206 - DEBUG - Extraction réussie: item: Centrifugal pump | item_french: Pompe centrifuge | category: Equipment | context: Used for transferring fluids by converting rotational kinetic energy into hydrodynamic energy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centrifugal pump suppliers in the Equipment category for Used for transferring fluids by converting rotational kinetic energy into hydrodynamic energy\n",
      "✅ All URLs Serper Search Results : 12\n",
      "https://mtsindustrie.ma/nos-solutions/solutions-de-pompage/\n",
      "https://toubkal.imist.ma/bitstream/handle/123456789/25587/264-20-BELBSIR%20HAMZA.pdf?sequence=1\n",
      "https://www.rotechpumps.com/types-of-centrifugal-pump/\n",
      "https://www.waterpump-cn.com/product/cpmcentrifugal/\n",
      "https://home.pumpsystemsacademy.com/blog/centrifugal-pumps\n",
      "http://www.hiseamarine.com/marine-centrifugal-pump/\n",
      "https://lincolnsuppliers.com/sanitary-centrifugal-pump/types-centrifugal-pumps-uses/\n",
      "https://www.dxpe.com/centrifugal-pumps-how-they-work-and-what-they-involve/\n",
      "https://en.wikipedia.org/wiki/Centrifugal_pump\n",
      "https://arroyoprocess.com/centrifugal-pumps/\n",
      "https://winstonengineering.com/guide-industrial-pump-types/\n",
      "https://www.trilliumflow.com/tf-news/vertical-pump-types-functions/\n",
      "🔍 Checking scrapability for search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:26:36,242 - INFO - ✅ https://mtsindustrie.ma/nos-solutions/solutions-de-pompage/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:36,395 - ERROR - ⚠️ Erreur d'accès à https://toubkal.imist.ma/bitstream/handle/123456789/25587/264-20-BELBSIR%20HAMZA.pdf?sequence=1: HTTPSConnectionPool(host='toubkal.imist.ma', port=443): Max retries exceeded with url: /bitstream/handle/123456789/25587/264-20-BELBSIR%20HAMZA.pdf?sequence=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL not accessible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:26:38,096 - INFO - ✅ https://www.rotechpumps.com/types-of-centrifugal-pump/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:41,274 - INFO - ✅ https://www.waterpump-cn.com/product/cpmcentrifugal/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:42,673 - INFO - ✅ https://home.pumpsystemsacademy.com/blog/centrifugal-pumps est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:44,519 - INFO - ✅ http://www.hiseamarine.com/marine-centrifugal-pump/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:45,810 - INFO - ✅ https://lincolnsuppliers.com/sanitary-centrifugal-pump/types-centrifugal-pumps-uses/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:46,770 - INFO - ✅ https://www.dxpe.com/centrifugal-pumps-how-they-work-and-what-they-involve/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:47,255 - INFO - ✅ https://en.wikipedia.org/wiki/Centrifugal_pump est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:52,105 - INFO - ✅ https://arroyoprocess.com/centrifugal-pumps/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:53,355 - INFO - ✅ https://winstonengineering.com/guide-industrial-pump-types/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:54,288 - INFO - ✅ https://www.trilliumflow.com/tf-news/vertical-pump-types-functions/ est scrapable sans JavaScript.\n",
      "2025-09-02 19:26:54,305 - INFO - 🧠 Résultats Serper récupérés: 11 liens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered URLs with total : 11\n",
      "✅ Filtered and deduplicated URLs ready for scraping with total : 11\n",
      "https://mtsindustrie.ma/nos-solutions/solutions-de-pompage/\n",
      "https://www.rotechpumps.com/types-of-centrifugal-pump/\n",
      "https://www.waterpump-cn.com/product/cpmcentrifugal/\n",
      "https://home.pumpsystemsacademy.com/blog/centrifugal-pumps\n",
      "http://www.hiseamarine.com/marine-centrifugal-pump/\n",
      "https://lincolnsuppliers.com/sanitary-centrifugal-pump/types-centrifugal-pumps-uses/\n",
      "https://www.dxpe.com/centrifugal-pumps-how-they-work-and-what-they-involve/\n",
      "https://en.wikipedia.org/wiki/Centrifugal_pump\n",
      "https://arroyoprocess.com/centrifugal-pumps/\n",
      "https://winstonengineering.com/guide-industrial-pump-types/\n",
      "https://www.trilliumflow.com/tf-news/vertical-pump-types-functions/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:26:58,722 - INFO - 📞 Contacts Kerix récupérés: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Aucun résultat trouvé sur Kerix.net.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:17,853 - ERROR - ❌ Thread échoué pour https://www.rotechpumps.com/types-of-centrifugal-pump/: name 'random' is not defined\n",
      "2025-09-02 19:27:17,926 - DEBUG - ✅ voici le principal Url: http://www.hiseamarine.com/marine-centrifugal-pump/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: HTTPSConnectionPool(host='www.rotechpumps.com', port=443): Read timed out. (read timeout=10). Retry in 1s...\n",
      "⚠️ voici Ses subUrl à analyser: http://www.hiseamarine.com/contacts.asp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:18,082 - DEBUG - ✅ voici le principal Url: https://mtsindustrie.ma/nos-solutions/solutions-de-pompage/\n",
      "2025-09-02 19:27:18,256 - DEBUG - ✅ voici le principal Url: https://www.waterpump-cn.com/product/cpmcentrifugal/\n",
      "2025-09-02 19:27:18,402 - DEBUG - ✅ voici le principal Url: https://home.pumpsystemsacademy.com/blog/centrifugal-pumps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ voici Ses subUrl à analyser: https://mtsindustrie.ma/contact/\n",
      "⚠️ voici Ses subUrl à analyser: https://mtsindustrie.ma/a-propos-de-nous/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.waterpump-cn.com/contact/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.waterpump-cn.com/about-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://home.pumpsystemsacademy.com/about\n",
      "⚠️ voici Ses subUrl à analyser: https://home.pumpsystemsacademy.com/contact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:19,258 - DEBUG - ✅ voici le principal Url: https://lincolnsuppliers.com/sanitary-centrifugal-pump/types-centrifugal-pumps-uses/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ voici Ses subUrl à analyser: https://lincolnsuppliers.com/contact/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:22,353 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: http://www.hiseamarine.com/marine-centrifugal-pump/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'Hi-Sea Group', 'email': 'manager@hiseamarine.com', 'phone': '+86-23-67956606', 'address': '', 'price': '', 'url': 'http://www.hiseamarine.com/marine-centrifugal-pump/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:22,853 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://home.pumpsystemsacademy.com/blog/centrifugal-pumps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'home.pumpsystemsacademy.com', 'email': '', 'phone': '', 'address': '', 'price': '', 'url': 'https://home.pumpsystemsacademy.com/blog/centrifugal-pumps'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:23,152 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://mtsindustrie.ma/nos-solutions/solutions-de-pompage/\n",
      "2025-09-02 19:27:23,273 - DEBUG - ✅ voici le principal Url: https://www.dxpe.com/centrifugal-pumps-how-they-work-and-what-they-involve/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'mtsindustrie.ma', 'email': 'contact@mtsindustrie.ma', 'phone': '+212 0523 32 69 66', 'address': 'Apt 8 Etg 2 Imm D Les Orchidees Mohammedia, MA 28820', 'price': '', 'url': 'https://mtsindustrie.ma/nos-solutions/solutions-de-pompage/'}\n",
      "⚠️ voici Ses subUrl à analyser: https://www.dxpe.com/about-us/corporate-sustainability/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.dxpe.com/about-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.dxpe.com/contact-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.dxpe.com/contact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:23,639 - DEBUG - ✅ voici le principal Url: https://en.wikipedia.org/wiki/Centrifugal_pump\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ voici Ses subUrl à analyser: https://en.wikipedia.org/wiki/wikipedia:contact_us\n",
      "⚠️ voici Ses subUrl à analyser: https://en.wikipedia.org/wiki/wikipedia:about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:23,869 - DEBUG - ✅ voici le principal Url: https://arroyoprocess.com/centrifugal-pumps/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ voici Ses subUrl à analyser: https://arroyoprocess.com/contact-us-s30/\n",
      "⚠️ voici Ses subUrl à analyser: https://arroyoprocess.com/contact-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://arroyoprocess.com/contact-us\n",
      "⚠️ voici Ses subUrl à analyser: https://arroyoprocess.com/about-us/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:24,719 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://lincolnsuppliers.com/sanitary-centrifugal-pump/types-centrifugal-pumps-uses/\n",
      "2025-09-02 19:27:24,999 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://www.waterpump-cn.com/product/cpmcentrifugal/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'lincolnsuppliers.com', 'email': '', 'phone': '800-622-8425', 'address': '', 'price': '', 'url': 'https://lincolnsuppliers.com/sanitary-centrifugal-pump/types-centrifugal-pumps-uses/'}\n",
      "{'part_number': '', 'name': 'waterpump-cn.com', 'email': 'sales18@elestarco.com', 'phone': '+86 18060570295', 'address': 'OFFICE:1301-03,SOHO BUILDING 1#,TAIHOT PLAZA,XINDIAN TOWN,JINAN DISTRICT,FUZHOU,FUJIAN,CHINA.', 'price': '$26.00', 'url': 'https://www.waterpump-cn.com/product/cpmcentrifugal/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:25,599 - DEBUG - ✅ voici le principal Url: https://www.trilliumflow.com/tf-news/vertical-pump-types-functions/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ voici Ses subUrl à analyser: https://www.trilliumflow.com/contact-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://trilliumflow.com/contact-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.trilliumflow.com/about/flowcast/\n",
      "⚠️ voici Ses subUrl à analyser: https://www.trilliumflow.com/about/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:27,040 - DEBUG - ✅ voici le principal Url: https://winstonengineering.com/guide-industrial-pump-types/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ voici Ses subUrl à analyser: https://winstonengineering.com/contact-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://contact/\n",
      "⚠️ voici Ses subUrl à analyser: https://winstonengineering.com/about-us/\n",
      "⚠️ voici Ses subUrl à analyser: https://winstonengineering.com/about/#core\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:27,871 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://en.wikipedia.org/wiki/Centrifugal_pump\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'Wikipedia', 'email': '', 'phone': '', 'address': '', 'price': '', 'url': 'https://en.wikipedia.org/wiki/Centrifugal_pump'}\n",
      "❌ Error: HTTPSConnectionPool(host='contact', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000021BE1AD4E50>: Failed to resolve 'contact' ([Errno 11001] getaddrinfo failed)\")). Retry in 1s...\n",
      "⚠️ Subpage scrape failed (https://contact/): name 'random' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:31,200 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://www.dxpe.com/centrifugal-pumps-how-they-work-and-what-they-involve/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'DXP', 'email': '', 'phone': '18008303973', 'address': '', 'price': '', 'url': 'https://www.dxpe.com/centrifugal-pumps-how-they-work-and-what-they-involve/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:31,482 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://arroyoprocess.com/centrifugal-pumps/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'Arroyo Process Equipment', 'email': '', 'phone': '+1 (863) 533-9700', 'address': '1550 Centennial Blvd. Bartow, FL 33830, 6635 Hwy Ave, Jacksonville, FL 32254, 2647 West 81st Street, Hialeah, FL 33016', 'price': '', 'url': 'https://arroyoprocess.com/centrifugal-pumps/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:32,997 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://www.trilliumflow.com/tf-news/vertical-pump-types-functions/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'Trillium Flow Technologies', 'email': '', 'phone': '+1 559 442 4000', 'address': '2495 S. Golden State Boulevard Fresno, CA 93706 USA', 'price': '', 'url': 'https://www.trilliumflow.com/tf-news/vertical-pump-types-functions/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 19:27:34,433 - DEBUG - ✅ voici le résultat de passage au LLM de l'Url: https://winstonengineering.com/guide-industrial-pump-types/\n",
      "127.0.0.1 - - [02/Sep/2025 19:27:34] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part_number': '', 'name': 'winstonengineering.com', 'email': '', 'phone': '', 'address': '', 'price': '', 'url': 'https://winstonengineering.com/guide-industrial-pump-types/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Sep/2025 19:27:34] \"GET /static/logo5.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Sep/2025 19:27:35] \"GET /static/background9.png HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=5002, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f381f677-3db9-4f42-8cf8-709f3c1d82bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
